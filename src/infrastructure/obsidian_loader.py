import logging
import os
from typing import List

from langchain_text_splitters import (
    MarkdownHeaderTextSplitter,
    RecursiveCharacterTextSplitter,
)

from src.core.models.domain_models import LifeEvent
from src.infrastructure.mem0_service import UserProfileService
from src.infrastructure.vector_store import KnowledgeBase

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MemoryIngestionEngine:
    def __init__(self, knowledge_base: KnowledgeBase):
        self.kb = knowledge_base
        self.mem0 = UserProfileService()

    def process_file(self, file_content: str, source_name: str = "unknown", save_to_mem0: bool = True) -> List[LifeEvent]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶å†…å®¹ (é€»è¾‘ä¿æŒä¸å˜)
        :param file_content: æ–‡ä»¶å†…å®¹
        :param source_name: æ–‡ä»¶å
        :param save_to_mem0: æ˜¯å¦ä¿å­˜åˆ° Mem0 (é»˜è®¤ True)
        """
        logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡ä»¶: {source_name} (é•¿åº¦: {len(file_content)} å­—ç¬¦)")

        # 1. ç»“æ„åŒ–åˆ‡åˆ† (æŒ‰æ ‡é¢˜)
        headers_to_split_on = [
            ("#", "Date/Title"),
            ("##", "Section"),
            ("###", "SubSection"),
        ]
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on
        )
        md_header_splits = markdown_splitter.split_text(file_content)
        logger.info(f"  â””â”€ ç»“æ„åŒ–åˆ‡åˆ†å®Œæˆ: {len(md_header_splits)} ä¸ªç‰‡æ®µ")

        # 2. é•¿åº¦åˆ‡åˆ†
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        final_splits = text_splitter.split_documents(md_header_splits)
        logger.info(f"  â””â”€ é•¿åº¦åˆ‡åˆ†å®Œæˆ: {len(final_splits)} ä¸ªå—")

        # 3. è½¬æ¢ä¸º LifeEvent
        life_events = []
        for doc in final_splits:
            event = LifeEvent(
                content=doc.page_content,
                source_type="obsidian",
                metadata={
                    "source_file": source_name,
                    **doc.metadata
                }
            )
            life_events.append(event)

        # 4. å­˜å…¥ä»“åº“
        if life_events:
            self.kb.add_events(life_events)
            logger.info(f"âœ… å·²ä¿å­˜ {len(life_events)} ä¸ªäº‹ä»¶åˆ°å‘é‡æ•°æ®åº“")
        else:
            logger.warning(f"âš ï¸ æœªä»æ–‡ä»¶ {source_name} ä¸­æå–åˆ°æœ‰æ•ˆå†…å®¹")

        if save_to_mem0:
            self.mem0.remember(file_content)

        return life_events

    def ingest_folder(self, folder_path: str, max_files: int = 100):
        """
        [æ–°å¢åŠŸèƒ½] é€’å½’æ‰«ææ–‡ä»¶å¤¹å¹¶å¯¼å…¥
        :param folder_path: Obsidian åº“çš„æ ¹ç›®å½•è·¯å¾„
        :param max_files: å®‰å…¨é™åˆ¶ï¼Œé˜²æ­¢ä¸€æ¬¡æ€§è¯»å…¥å‡ åƒä¸ªæ–‡ä»¶æŠŠé’±çƒ§å…‰
        """
        logger.info(f"ğŸ“‚ [Loader] å¼€å§‹æ‰«æç›®å½•: {folder_path}")

        if not os.path.exists(folder_path):
            error_msg = f"è·¯å¾„ä¸å­˜åœ¨: {folder_path}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        processed_count = 0
        memories_to_add = []
        batch_size = 5  # Batch size for Mem0 to avoid context limits

        # os.walk é€’å½’éå†æ‰€æœ‰å­ç›®å½•
        for root, dirs, files in os.walk(folder_path):
            # è¿‡æ»¤æ‰éšè—æ–‡ä»¶å¤¹ (å¦‚ .obsidian, .git)
            dirs[:] = [d for d in dirs if not d.startswith('.')]

            for file in files:
                if processed_count >= max_files:
                    logger.warning(f"ğŸ›‘ [Loader] è¾¾åˆ°æœ€å¤§æ–‡ä»¶é™åˆ¶ ({max_files})ï¼Œåœæ­¢åŠ è½½ã€‚")
                    break

                if file.endswith(".md"):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            content = f.read()

                        # è·å–ç›¸å¯¹è·¯å¾„ä½œä¸º source_name (ä¾‹å¦‚: "Work/2023-10-10.md")
                        relative_path = os.path.relpath(file_path, folder_path)

                        # è°ƒç”¨ä¹‹å‰çš„å•æ–‡ä»¶å¤„ç†é€»è¾‘, ä½†ä¸ä¿å­˜åˆ° Mem0
                        self.process_file(content, source_name=relative_path, save_to_mem0=False)

                        memories_to_add.append(content)
                        processed_count += 1
                        logger.info(f"âœ… [{processed_count}] å·²å¤„ç†: {relative_path}")

                        # Batch process mem0
                        if len(memories_to_add) >= batch_size:
                            self.mem0.remember(memories_to_add)
                            memories_to_add = []

                    except Exception as e:
                        error_msg = f"è·³è¿‡æ–‡ä»¶ {file}: {e}"
                        logger.warning(error_msg)

            if processed_count >= max_files:
                 break

        # Process remaining memories
        if memories_to_add:
            self.mem0.remember(memories_to_add)

        logger.info(f"ğŸ‰ [Loader] æ‰¹é‡å¯¼å…¥å®Œæˆï¼Œå…±å¤„ç† {processed_count} ä¸ªæ–‡ä»¶ã€‚")
