import logging
import os
from concurrent.futures import ThreadPoolExecutor
from typing import List

from langchain_text_splitters import (
    MarkdownHeaderTextSplitter,
    RecursiveCharacterTextSplitter,
)

from src.core.models.domain_models import LifeEvent
from src.infrastructure.mem0_service import UserProfileService
from src.infrastructure.vector_store import KnowledgeBase

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MemoryIngestionEngine:
    def __init__(self, knowledge_base: KnowledgeBase):
        self.kb = knowledge_base
        self.mem0 = UserProfileService()

    def process_file(self, file_content: str, source_name: str = "unknown") -> List[LifeEvent]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶å†…å®¹ (é€»è¾‘ä¿æŒä¸å˜)
        """
        logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡ä»¶: {source_name} (é•¿åº¦: {len(file_content)} å­—ç¬¦)")

        # 1. ç»“æ„åŒ–åˆ‡åˆ† (æŒ‰æ ‡é¢˜)
        headers_to_split_on = [
            ("#", "Date/Title"),
            ("##", "Section"),
            ("###", "SubSection"),
        ]
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on
        )
        md_header_splits = markdown_splitter.split_text(file_content)
        logger.info(f"  â””â”€ ç»“æ„åŒ–åˆ‡åˆ†å®Œæˆ: {len(md_header_splits)} ä¸ªç‰‡æ®µ")

        # 2. é•¿åº¦åˆ‡åˆ†
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        final_splits = text_splitter.split_documents(md_header_splits)
        logger.info(f"  â””â”€ é•¿åº¦åˆ‡åˆ†å®Œæˆ: {len(final_splits)} ä¸ªå—")

        # 3. è½¬æ¢ä¸º LifeEvent
        life_events = []
        for doc in final_splits:
            event = LifeEvent(
                content=doc.page_content,
                source_type="obsidian",
                metadata={
                    "source_file": source_name,
                    **doc.metadata
                }
            )
            life_events.append(event)

        # 4. å­˜å…¥ä»“åº“ (å¹¶å‘æ‰§è¡Œ: å‘é‡æ•°æ®åº“ + ç”¨æˆ·ç”»åƒ)
        # âš¡ Bolt Optimization: Run independent IO-bound tasks in parallel
        with ThreadPoolExecutor(max_workers=2) as executor:
            kb_future = None
            if life_events:
                kb_future = executor.submit(self.kb.add_events, life_events)
            else:
                logger.warning(f"âš ï¸ æœªä»æ–‡ä»¶ {source_name} ä¸­æå–åˆ°æœ‰æ•ˆå†…å®¹")

            mem0_future = executor.submit(self.mem0.remember, file_content)

            # Check Task A (KB) result
            if kb_future:
                try:
                    kb_future.result()
                    logger.info(f"âœ… å·²ä¿å­˜ {len(life_events)} ä¸ªäº‹ä»¶åˆ°å‘é‡æ•°æ®åº“")
                except Exception as e:
                    logger.error(f"âŒ Error saving to KnowledgeBase: {e}")
                    raise e  # Re-raise to prevent false success

            # Check Task B (Mem0) result
            try:
                mem0_future.result()
            except Exception as e:
                logger.error(f"âŒ Error updating User Profile: {e}")
                # We might choose not to fail the whole process if Mem0 fails,
                # but for now let's be strict or at least log it clearly.
                # Continuing despite Mem0 failure is acceptable if KB succeeded.

        return life_events

    def ingest_folder(self, folder_path: str, max_files: int = 100):
        """
        [æ–°å¢åŠŸèƒ½] é€’å½’æ‰«ææ–‡ä»¶å¤¹å¹¶å¯¼å…¥
        :param folder_path: Obsidian åº“çš„æ ¹ç›®å½•è·¯å¾„
        :param max_files: å®‰å…¨é™åˆ¶ï¼Œé˜²æ­¢ä¸€æ¬¡æ€§è¯»å…¥å‡ åƒä¸ªæ–‡ä»¶æŠŠé’±çƒ§å…‰
        """
        logger.info(f"ğŸ“‚ [Loader] å¼€å§‹æ‰«æç›®å½•: {folder_path}")

        if not os.path.exists(folder_path):
            error_msg = f"è·¯å¾„ä¸å­˜åœ¨: {folder_path}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        processed_count = 0

        # os.walk é€’å½’éå†æ‰€æœ‰å­ç›®å½•
        for root, dirs, files in os.walk(folder_path):
            # è¿‡æ»¤æ‰éšè—æ–‡ä»¶å¤¹ (å¦‚ .obsidian, .git)
            dirs[:] = [d for d in dirs if not d.startswith('.')]

            for file in files:
                if processed_count >= max_files:
                    logger.warning(f"ğŸ›‘ [Loader] è¾¾åˆ°æœ€å¤§æ–‡ä»¶é™åˆ¶ ({max_files})ï¼Œåœæ­¢åŠ è½½ã€‚")
                    return

                if file.endswith(".md"):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            content = f.read()

                        # è·å–ç›¸å¯¹è·¯å¾„ä½œä¸º source_name (ä¾‹å¦‚: "Work/2023-10-10.md")
                        relative_path = os.path.relpath(file_path, folder_path)

                        # è°ƒç”¨ä¹‹å‰çš„å•æ–‡ä»¶å¤„ç†é€»è¾‘
                        self.process_file(content, source_name=relative_path)
                        processed_count += 1
                        logger.info(f"âœ… [{processed_count}] å·²å¤„ç†: {relative_path}")

                    except Exception as e:
                        error_msg = f"è·³è¿‡æ–‡ä»¶ {file}: {e}"
                        logger.warning(error_msg)

        logger.info(f"ğŸ‰ [Loader] æ‰¹é‡å¯¼å…¥å®Œæˆï¼Œå…±å¤„ç† {processed_count} ä¸ªæ–‡ä»¶ã€‚")
