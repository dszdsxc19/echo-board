import os

from langchain_ollama import OllamaEmbeddings
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from src.infrastructure.vector_store import vector_store, embeddings
# ==========================================
# 1. æ¨¡æ‹Ÿæ•°æ® (Mock Data for MVP)
# åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œè¿™é‡Œä¼šæ›¿æ¢ä¸ºè¯»å–ä½ çš„ Obsidian æ–‡ä»¶å¤¹
# ==========================================
OBSIDIAN_MOCK_CONTENT = """
# 2023-10-10 å·¥ä½œå¤ç›˜

## é¡¹ç›® A çš„åæ€
ä»Šå¤©é¡¹ç›® A çš„è¿›åº¦éå¸¸æ»åã€‚ä¸»è¦åŸå› æ˜¯åœ¨æ¶æ„é€‰å‹ä¸ŠçŠ¹è±«å¤ªä¹…ã€‚
æˆ‘è®¤ä¸ºæˆ‘ä»¬éœ€è¦é‡æ–°è¯„ä¼° Go è¯­è¨€åœ¨ç›®å‰çš„é€‚ç”¨æ€§ã€‚
ç›®å‰å›¢é˜Ÿå¯¹ Go çš„æŒæ¡ç¨‹åº¦ä¸å¤Ÿï¼Œå¯¼è‡´å¼€å‘æ•ˆç‡ä½ä¸‹ã€‚

## å¾…åŠæ¸…å•
- è®°å¾—ä¹°çŒ«ç²®
- é¢„çº¦ç‰™åŒ»
- è¯»ã€Šè½¯ä»¶è®¾è®¡ä¹‹ç¾ã€‹ç¬¬3ç« 

# 2023-10-11 å¿ƒæƒ…æ—¥è®°

## ç„¦è™‘æ—¶åˆ»
æ˜¨æ™šå¤±çœ äº†ï¼Œä¸€ç›´åœ¨æƒ³æˆ¿è´·çš„äº‹æƒ…ã€‚
æ„Ÿè§‰ç°åœ¨çš„æ”¶å…¥ç»“æ„å¤ªå•ä¸€ï¼ŒæŠ—é£é™©èƒ½åŠ›å·®ã€‚
"""

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ï¼šç»“æ„åŒ–åˆ‡åˆ† (The Ingestion Logic)
# ==========================================
class MemoryIngestionEngine:
    def __init__(self, vector_store: Chroma, embeddings: OllamaEmbeddings):
        # åˆå§‹åŒ– Embedding æ¨¡å‹ (è¿™é‡Œå‡è®¾ä½ é…ç½®å¥½äº† OPENAI_API_KEY ç¯å¢ƒå˜é‡)
        self.embeddings = embeddings
        self.vector_store = vector_store

    def process_markdown(self, markdown_text):
        """
        æ ¸å¿ƒç®—æ³•ï¼šåˆ©ç”¨ Markdown æ ‡é¢˜ä¿ç•™ä¸Šä¸‹æ–‡
        """
        # A. å®šä¹‰æˆ‘ä»¬è¦åˆ‡åˆ†çš„å±‚çº§
        headers_to_split_on = [
            ("#", "Date/Title"),
            ("##", "Section"),
            ("###", "SubSection"),
        ]

        # B. ç¬¬ä¸€åˆ€ï¼šæŒ‰æ ‡é¢˜åˆ‡åˆ† (ä¿ç•™ç»“æ„å…ƒæ•°æ®)
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on
        )
        md_header_splits = markdown_splitter.split_text(markdown_text)

        # C. ç¬¬äºŒåˆ€ï¼šæŒ‰å­—ç¬¦é•¿åº¦åˆ‡åˆ† (é˜²æ­¢é•¿æ–‡æº¢å‡ºï¼ŒåŒæ—¶ä¿ç•™æ ‡é¢˜å…ƒæ•°æ®)
        # è¿™ä¸€æ­¥å¯¹äº "é¡¹ç›® A çš„åæ€" è¿™ç§é•¿æ®µè½å¾ˆé‡è¦
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        final_splits = text_splitter.split_documents(md_header_splits)

        # D. æ³¨å…¥ Source Type (ä¸ºæœªæ¥æ‰©å±•åšå‡†å¤‡)
        for doc in final_splits:
            doc.metadata["source_type"] = "obsidian_note"

        print(f"âœ… æˆåŠŸåˆ‡åˆ†ä¸º {len(final_splits)} ä¸ªè®°å¿†ç‰‡æ®µ")
        return final_splits

    def save_to_memory(self, chunks):
        """å­˜å…¥å‘é‡åº“"""
        self.vector_store.add_documents(chunks)
        print("ğŸ’¾ å·²å­˜å…¥å‘é‡æ•°æ®åº“")

    def search(self, query, top_k=3):
        """
        å²å®˜çš„é›å½¢ï¼šæ£€ç´¢æ¥å£
        """
        results = self.vector_store.similarity_search_with_score(query, k=top_k)
        return results

# ==========================================
# 3. è¿è¡Œæµ‹è¯• (Verify the Core)
# ==========================================
if __name__ == "__main__":
    # A. å¯åŠ¨å¼•æ“
    engine = MemoryIngestionEngine(vector_store=vector_store, embeddings=embeddings)

    # B. æ³¨å…¥æ•°æ®
    print("--- æ­£åœ¨å¤„ç†æ•°æ® ---")
    chunks = engine.process_markdown(OBSIDIAN_MOCK_CONTENT)
    
    # æ‰“å°ä¸€ä¸‹åˆ‡åˆ†ç»“æœï¼Œçœ‹çœ‹å…ƒæ•°æ®æ˜¯å¦ä¿ç•™äº† (å…³é”®éªŒè¯ç‚¹!)
    print("\n[åˆ‡åˆ†æ ·æœ¬æŸ¥çœ‹]:")
    print(f"å†…å®¹: {chunks[1].page_content}")
    print(f"å…ƒæ•°æ®: {chunks[1].metadata}") 
    # é¢„æœŸè¾“å‡º metadata: {'Date/Title': '2023-10-10 å·¥ä½œå¤ç›˜', 'Section': 'å¾…åŠæ¸…å•', ...}
    
    engine.save_to_memory(chunks)

    # C. æ¨¡æ‹Ÿ Agent æ£€ç´¢
    print("\n--- æ¨¡æ‹Ÿ Agent æ£€ç´¢ ---")
    
    # æµ‹è¯• 1: æ¨¡ç³Šæƒ…æ„Ÿæ£€ç´¢
    query1 = "æˆ‘æœ€è¿‘ä¸ºä»€ä¹ˆæ„Ÿåˆ°å‹åŠ›å¤§ï¼Ÿ"
    print(f"\nğŸ” Query: {query1}")
    results1 = engine.search(query1, top_k=3)
    for res, score in results1:
        print(f"- [åŒ¹é…åº¦] {res.page_content[:50]}... (æ¥è‡ª: {res.metadata.get('Date/Title')} > {res.metadata.get('Section')}) [SIM={score:3f}]")

    # æµ‹è¯• 2: å…·ä½“äº‹åŠ¡æ£€ç´¢
    query2 = "æˆ‘è¦ä¹°ä»€ä¹ˆç»™å® ç‰©ï¼Ÿ"
    print(f"\nğŸ” Query: {query2}")
    results2 = engine.search(query2, top_k=3)
    for res, score in results2:
        print(f"- [åŒ¹é…åº¦] {res.page_content[:50]}... (æ¥è‡ª: {res.metadata.get('Section')}) [SIM={score:3f}]")

